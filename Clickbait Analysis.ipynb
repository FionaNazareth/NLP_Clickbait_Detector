{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b3e545",
   "metadata": {},
   "source": [
    "* Dataset: https://www.kaggle.com/amananandrai/clickbait-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b5eef",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dfb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c99139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d97d5",
   "metadata": {},
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e964c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_data_actual = pd.read_csv(\"Data/clickbait_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8ad19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait\n",
       "0                                 Should I Get Bings          1\n",
       "1      Which TV Female Friend Group Do You Belong In          1\n",
       "2  The New \"Star Wars: The Force Awakens\" Trailer...          1\n",
       "3  This Vine Of New York On \"Celebrity Big Brothe...          1\n",
       "4  A Couple Did A Stunning Photo Shoot With Their...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_data_actual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc806a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32000 entries, 0 to 31999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   headline   32000 non-null  object\n",
      " 1   clickbait  32000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 500.1+ KB\n"
     ]
    }
   ],
   "source": [
    "click_data_actual.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab902c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clickbait = click_data_actual[click_data_actual['clickbait'] == 1].head(8000)\n",
    "data_no_clickbait = click_data_actual[click_data_actual['clickbait'] == 0].head(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fc7d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>A Good Run for Mutual Funds, but Questions Remain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>Circus elephant escapes in Zurich, Switzerland</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>Bush backtracks over legal status of alleged \"...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>Synchronised bombings strike Indian state of A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>U.S. Agencies Advised to Release Information t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  clickbait\n",
       "15995  A Good Run for Mutual Funds, but Questions Remain          0\n",
       "15996     Circus elephant escapes in Zurich, Switzerland          0\n",
       "15997  Bush backtracks over legal status of alleged \"...          0\n",
       "15998  Synchronised bombings strike Indian state of A...          0\n",
       "15999  U.S. Agencies Advised to Release Information t...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_data = pd.concat([data_clickbait, data_no_clickbait])\n",
    "click_data = click_data.reset_index(drop=True)\n",
    "click_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ffc2e",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d99e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data\n",
    "click_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174361b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8000\n",
       "1    8000\n",
       "Name: clickbait, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many records are clickbait and how many are not?\n",
    "click_data['clickbait'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ebd717c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline     0\n",
       "clickbait    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find missing data\n",
    "click_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae11e576",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf68ef5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca9903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text_no_punctuation = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dabdd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_data['headline_no_punctuation'] = click_data['headline'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "#click_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf920be",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e4abb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function tokenizes the text by sliptting the text at places where it finds a non word/ non numeric character\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "605df11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_data['headline_tokenized'] = click_data['headline_no_punctuation'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "#click_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4536cb",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74efdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10400bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8723a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_lst):\n",
    "    text = [word for word in tokenized_lst if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92b53ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_data[\"headline_no_stopwords\"] = click_data[\"headline_tokenized\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#click_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b3ff6",
   "metadata": {},
   "source": [
    "### Stem text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d98c00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PorterStemmer\n",
    "# ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f871966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stemming(tokenized_text):\n",
    "#    stem_text = \" \".join(ps.stem(word) for word in tokenized_text)\n",
    "#    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3afeb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click_data['headline_stemmed'] = click_data['headline_no_stopwords'].apply(lambda x: stemming(x))\n",
    "\n",
    "#click_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ddea5",
   "metadata": {},
   "source": [
    "### Lemmatize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9298c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ede39fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    lemmatized_text = \" \".join(wn.lemmatize(word) for word in tokenized_text)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a29b42d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "click_data['headline_lemmatized'] = click_data['headline_no_stopwords'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "#click_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d91805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   headline                 16000 non-null  object\n",
      " 1   clickbait                16000 non-null  int64 \n",
      " 2   headline_no_punctuation  16000 non-null  object\n",
      " 3   headline_tokenized       16000 non-null  object\n",
      " 4   headline_no_stopwords    16000 non-null  object\n",
      " 5   headline_lemmatized      16000 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 750.1+ KB\n"
     ]
    }
   ],
   "source": [
    "click_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddafe645",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efded1f4",
   "metadata": {},
   "source": [
    "## <center>Vectorization</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6cf7e",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c877df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5c6c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8552be6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "      <th>headline_no_punctuation</th>\n",
       "      <th>headline_tokenized</th>\n",
       "      <th>headline_no_stopwords</th>\n",
       "      <th>headline_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>[should, i, get, bings]</td>\n",
       "      <td>[get, bings]</td>\n",
       "      <td>get bings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>[which, tv, female, friend, group, do, you, be...</td>\n",
       "      <td>[tv, female, friend, group, belong]</td>\n",
       "      <td>tv female friend group belong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "      <td>The New Star Wars The Force Awakens Trailer Is...</td>\n",
       "      <td>[the, new, star, wars, the, force, awakens, tr...</td>\n",
       "      <td>[new, star, wars, force, awakens, trailer, giv...</td>\n",
       "      <td>new star war force awakens trailer give chill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "      <td>This Vine Of New York On Celebrity Big Brother...</td>\n",
       "      <td>[this, vine, of, new, york, on, celebrity, big...</td>\n",
       "      <td>[vine, new, york, celebrity, big, brother, fuc...</td>\n",
       "      <td>vine new york celebrity big brother fucking pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>[a, couple, did, a, stunning, photo, shoot, wi...</td>\n",
       "      <td>[couple, stunning, photo, shoot, baby, learnin...</td>\n",
       "      <td>couple stunning photo shoot baby learning inop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait  \\\n",
       "0                                 Should I Get Bings          1   \n",
       "1      Which TV Female Friend Group Do You Belong In          1   \n",
       "2  The New \"Star Wars: The Force Awakens\" Trailer...          1   \n",
       "3  This Vine Of New York On \"Celebrity Big Brothe...          1   \n",
       "4  A Couple Did A Stunning Photo Shoot With Their...          1   \n",
       "\n",
       "                             headline_no_punctuation  \\\n",
       "0                                 Should I Get Bings   \n",
       "1      Which TV Female Friend Group Do You Belong In   \n",
       "2  The New Star Wars The Force Awakens Trailer Is...   \n",
       "3  This Vine Of New York On Celebrity Big Brother...   \n",
       "4  A Couple Did A Stunning Photo Shoot With Their...   \n",
       "\n",
       "                                  headline_tokenized  \\\n",
       "0                            [should, i, get, bings]   \n",
       "1  [which, tv, female, friend, group, do, you, be...   \n",
       "2  [the, new, star, wars, the, force, awakens, tr...   \n",
       "3  [this, vine, of, new, york, on, celebrity, big...   \n",
       "4  [a, couple, did, a, stunning, photo, shoot, wi...   \n",
       "\n",
       "                               headline_no_stopwords  \\\n",
       "0                                       [get, bings]   \n",
       "1                [tv, female, friend, group, belong]   \n",
       "2  [new, star, wars, force, awakens, trailer, giv...   \n",
       "3  [vine, new, york, celebrity, big, brother, fuc...   \n",
       "4  [couple, stunning, photo, shoot, baby, learnin...   \n",
       "\n",
       "                                 headline_lemmatized  \n",
       "0                                          get bings  \n",
       "1                      tv female friend group belong  \n",
       "2      new star war force awakens trailer give chill  \n",
       "3  vine new york celebrity big brother fucking pe...  \n",
       "4  couple stunning photo shoot baby learning inop...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = click_data.copy()\n",
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4da5bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count = count_vect.fit_transform(data_1['headline_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd596dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 15389)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce927a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '00s',\n",
       " '01',\n",
       " '02',\n",
       " '04',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '10000copy',\n",
       " '10000th',\n",
       " '100k',\n",
       " '100th',\n",
       " '100yearold',\n",
       " '100ºf',\n",
       " '101',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '10600',\n",
       " '108',\n",
       " '109',\n",
       " '109th',\n",
       " '10hour',\n",
       " '10minute',\n",
       " '10second',\n",
       " '10th',\n",
       " '10year',\n",
       " '10yearold',\n",
       " '11',\n",
       " '1100',\n",
       " '111',\n",
       " '113',\n",
       " '115',\n",
       " '117',\n",
       " '119',\n",
       " '11k',\n",
       " '11th',\n",
       " '11yearolds',\n",
       " '12',\n",
       " '1200',\n",
       " '12000',\n",
       " '120000',\n",
       " '1200mile',\n",
       " '121',\n",
       " '1215',\n",
       " '126292pound',\n",
       " '12th',\n",
       " '12yearold',\n",
       " '13',\n",
       " '130',\n",
       " '13000',\n",
       " '133',\n",
       " '134',\n",
       " '13500',\n",
       " '136',\n",
       " '137',\n",
       " '139',\n",
       " '139b',\n",
       " '13yearold',\n",
       " '14',\n",
       " '140',\n",
       " '14000',\n",
       " '140000',\n",
       " '14000acre',\n",
       " '1419',\n",
       " '144',\n",
       " '146',\n",
       " '147',\n",
       " '149',\n",
       " '14yearold',\n",
       " '15',\n",
       " '150',\n",
       " '15000',\n",
       " '153',\n",
       " '153000',\n",
       " '154',\n",
       " '159',\n",
       " '15m',\n",
       " '15second',\n",
       " '15yearold',\n",
       " '16',\n",
       " '160',\n",
       " '163',\n",
       " '164',\n",
       " '165',\n",
       " '168',\n",
       " '16death',\n",
       " '16hour',\n",
       " '16th',\n",
       " '16year',\n",
       " '16yearold',\n",
       " '17',\n",
       " '1700',\n",
       " '172',\n",
       " '175',\n",
       " '17point',\n",
       " '17th',\n",
       " '17year',\n",
       " '17yearold',\n",
       " '18',\n",
       " '180',\n",
       " '1850',\n",
       " '188',\n",
       " '18myearold',\n",
       " '18story',\n",
       " '18th',\n",
       " '18year',\n",
       " '19',\n",
       " '19000',\n",
       " '1900s',\n",
       " '191',\n",
       " '1915',\n",
       " '1919',\n",
       " '1920s',\n",
       " '1930',\n",
       " '1930s',\n",
       " '19391993',\n",
       " '1945',\n",
       " '195',\n",
       " '1950s',\n",
       " '1955',\n",
       " '1957',\n",
       " '1959',\n",
       " '1964',\n",
       " '1970s',\n",
       " '1976',\n",
       " '1978',\n",
       " '1980s',\n",
       " '1982',\n",
       " '1984',\n",
       " '1985',\n",
       " '1989',\n",
       " '199',\n",
       " '1990s',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '19yearold',\n",
       " '1st',\n",
       " '1year',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '20000',\n",
       " '200000',\n",
       " '2000s',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '20042005',\n",
       " '2005',\n",
       " '20050108',\n",
       " '2006',\n",
       " '2007',\n",
       " '200708',\n",
       " '2008',\n",
       " '20082009',\n",
       " '2009',\n",
       " '200k',\n",
       " '200s',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '20132014',\n",
       " '2014',\n",
       " '2015',\n",
       " '2015s',\n",
       " '2016',\n",
       " '2018',\n",
       " '2019',\n",
       " '2020',\n",
       " '2050',\n",
       " '2089',\n",
       " '20k',\n",
       " '20something',\n",
       " '20th',\n",
       " '20yearold',\n",
       " '21',\n",
       " '2115',\n",
       " '215',\n",
       " '21yearold',\n",
       " '22',\n",
       " '222',\n",
       " '222570',\n",
       " '225',\n",
       " '226',\n",
       " '22year',\n",
       " '22yearold',\n",
       " '23',\n",
       " '234',\n",
       " '235',\n",
       " '23rd',\n",
       " '23yearold',\n",
       " '24',\n",
       " '240',\n",
       " '24hour',\n",
       " '24yearold',\n",
       " '25',\n",
       " '250',\n",
       " '25000',\n",
       " '250000',\n",
       " '258',\n",
       " '25m',\n",
       " '25yearold',\n",
       " '26',\n",
       " '2611',\n",
       " '263',\n",
       " '265th',\n",
       " '266',\n",
       " '266m',\n",
       " '26hour',\n",
       " '26year',\n",
       " '27',\n",
       " '270',\n",
       " '28',\n",
       " '280',\n",
       " '29',\n",
       " '292',\n",
       " '299',\n",
       " '29th',\n",
       " '29yearolds',\n",
       " '2channel',\n",
       " '2gb',\n",
       " '2k15',\n",
       " '2nd',\n",
       " '2ndlargest',\n",
       " '2year',\n",
       " '2yearold',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30000',\n",
       " '300000',\n",
       " '300th',\n",
       " '300ton',\n",
       " '306',\n",
       " '30a',\n",
       " '30yearold',\n",
       " '31',\n",
       " '31948',\n",
       " '32',\n",
       " '3200',\n",
       " '320000',\n",
       " '32500',\n",
       " '33',\n",
       " '333',\n",
       " '34',\n",
       " '34m',\n",
       " '34year',\n",
       " '35',\n",
       " '350',\n",
       " '3500',\n",
       " '351',\n",
       " '35th',\n",
       " '36',\n",
       " '360',\n",
       " '36bn',\n",
       " '37',\n",
       " '378',\n",
       " '38',\n",
       " '38m',\n",
       " '38s',\n",
       " '39',\n",
       " '39k',\n",
       " '3c',\n",
       " '3dprinted',\n",
       " '3g',\n",
       " '3in1',\n",
       " '3ingredient',\n",
       " '3nd',\n",
       " '3rd',\n",
       " '3yearold',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '4000th',\n",
       " '41',\n",
       " '41000',\n",
       " '42',\n",
       " '4200',\n",
       " '43',\n",
       " '4317',\n",
       " '435',\n",
       " '44',\n",
       " '440',\n",
       " '440000',\n",
       " '447',\n",
       " '44th',\n",
       " '45',\n",
       " '450',\n",
       " '45kilometre',\n",
       " '45yearold',\n",
       " '46',\n",
       " '468',\n",
       " '46passenger',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '4th',\n",
       " '4thquarter',\n",
       " '4yearold',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50000',\n",
       " '500000',\n",
       " '50b',\n",
       " '50th',\n",
       " '51',\n",
       " '5191',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '56magnitude',\n",
       " '56to1',\n",
       " '57',\n",
       " '574',\n",
       " '58',\n",
       " '59',\n",
       " '5aside',\n",
       " '5day',\n",
       " '5foot9',\n",
       " '5p',\n",
       " '5second',\n",
       " '5sos',\n",
       " '5th',\n",
       " '60',\n",
       " '600',\n",
       " '60000',\n",
       " '60th',\n",
       " '60year',\n",
       " '61',\n",
       " '616',\n",
       " '62',\n",
       " '620',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '6500',\n",
       " '65yearold',\n",
       " '66',\n",
       " '67',\n",
       " '67th',\n",
       " '68',\n",
       " '69',\n",
       " '69ing',\n",
       " '6footsomething',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '7000',\n",
       " '70th',\n",
       " '70yearold',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '737',\n",
       " '737s',\n",
       " '73myearold',\n",
       " '74',\n",
       " '745',\n",
       " '75',\n",
       " '7500',\n",
       " '757',\n",
       " '75w',\n",
       " '76',\n",
       " '77',\n",
       " '775',\n",
       " '777',\n",
       " '78',\n",
       " '78000',\n",
       " '787',\n",
       " '788000',\n",
       " '789',\n",
       " '78th',\n",
       " '79',\n",
       " '7day',\n",
       " '7m',\n",
       " '7th',\n",
       " '7yearold',\n",
       " '80',\n",
       " '800',\n",
       " '8000',\n",
       " '80000',\n",
       " '800th',\n",
       " '8020',\n",
       " '80211n',\n",
       " '80th',\n",
       " '81',\n",
       " '82',\n",
       " '821',\n",
       " '823',\n",
       " '82nd',\n",
       " '83',\n",
       " '84',\n",
       " '8400',\n",
       " '85',\n",
       " '86',\n",
       " '8600',\n",
       " '87',\n",
       " '876',\n",
       " '88',\n",
       " '89',\n",
       " '8995',\n",
       " '89yearold',\n",
       " '8picture',\n",
       " '8yearold',\n",
       " '90',\n",
       " '900',\n",
       " '9000',\n",
       " '90000',\n",
       " '90210',\n",
       " '90th',\n",
       " '90yearold',\n",
       " '91',\n",
       " '911',\n",
       " '918',\n",
       " '92',\n",
       " '92yearold',\n",
       " '93',\n",
       " '93yearold',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '995',\n",
       " '9month',\n",
       " '9yearold',\n",
       " 'a1gp',\n",
       " 'a320',\n",
       " 'a340',\n",
       " 'a380',\n",
       " 'a400m',\n",
       " 'aaa',\n",
       " 'aaevpc',\n",
       " 'aap',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abba',\n",
       " 'abbas',\n",
       " 'abbey',\n",
       " 'abbott',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abdallahi',\n",
       " 'abdelbaset',\n",
       " 'abdicates',\n",
       " 'abducted',\n",
       " 'abduction',\n",
       " 'abductor',\n",
       " 'abdullah',\n",
       " 'abel',\n",
       " 'abercrombie',\n",
       " 'aberdeen',\n",
       " 'abhishek',\n",
       " 'ability',\n",
       " 'abin',\n",
       " 'abitibibowater',\n",
       " 'abkhazia',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'aborigine',\n",
       " 'abortion',\n",
       " 'aboulhosn',\n",
       " 'abound',\n",
       " 'abraham',\n",
       " 'abramoff',\n",
       " 'abrams',\n",
       " 'abroad',\n",
       " 'abrogates',\n",
       " 'absence',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorbs',\n",
       " 'abstinence',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'abu',\n",
       " 'abuela',\n",
       " 'abuelita',\n",
       " 'abuelitas',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuserelated',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'accelerated',\n",
       " 'accelerating',\n",
       " 'acceleration',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accord',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuses',\n",
       " 'acdcs',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'achievement',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'acid',\n",
       " 'acknowledge',\n",
       " 'aclu',\n",
       " 'acne',\n",
       " 'acorn',\n",
       " 'acoustic',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquires',\n",
       " 'acquisition',\n",
       " 'acquit',\n",
       " 'acquits',\n",
       " 'acquitted',\n",
       " 'acre',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actionadventure',\n",
       " 'actionpacked',\n",
       " 'activated',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activist',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acutely',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adapt',\n",
       " 'adapted',\n",
       " 'adcenter',\n",
       " 'add',\n",
       " 'addadhd',\n",
       " 'addams',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'addington',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'adelaide',\n",
       " 'adele',\n",
       " 'adeles',\n",
       " 'adelphia',\n",
       " 'adenhart',\n",
       " 'adfree',\n",
       " 'adha',\n",
       " 'adhd',\n",
       " 'adidas',\n",
       " 'adirondacks',\n",
       " 'adjust',\n",
       " 'adjustment',\n",
       " 'adkins',\n",
       " 'administration',\n",
       " 'administrator',\n",
       " 'admiral',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adolescent',\n",
       " 'adolf',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adopts',\n",
       " 'adorable',\n",
       " 'adorably',\n",
       " 'adore',\n",
       " 'adrian',\n",
       " 'adrienne',\n",
       " 'adrift',\n",
       " 'aduba',\n",
       " 'adult',\n",
       " 'adulthood',\n",
       " 'adulting',\n",
       " 'adulty',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advent',\n",
       " 'adventure',\n",
       " 'adventurer',\n",
       " 'adventurous',\n",
       " 'advert',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertiser',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'adviser',\n",
       " 'advisor',\n",
       " 'advisory',\n",
       " 'advocacy',\n",
       " 'advocate',\n",
       " 'aer',\n",
       " 'aerial',\n",
       " 'aerodynamics',\n",
       " 'aeroflotnord',\n",
       " 'aerosmith',\n",
       " 'af',\n",
       " 'afc',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affirms',\n",
       " 'affleck',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afghan',\n",
       " 'afghanistan',\n",
       " 'afghanistanpakistan',\n",
       " 'afl',\n",
       " 'afleet',\n",
       " 'afloat',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africanamerican',\n",
       " 'afrolatinos',\n",
       " 'afterchristmas',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afterparty',\n",
       " 'aftershock',\n",
       " 'agassi',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'ageism',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'aggravated',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'agim',\n",
       " 'aging',\n",
       " 'agniiii',\n",
       " 'ago',\n",
       " 'agonizing',\n",
       " 'agree',\n",
       " 'agreement',\n",
       " 'agrees',\n",
       " 'agriculture',\n",
       " 'aground',\n",
       " 'aguilera',\n",
       " 'ahead',\n",
       " 'ahhhhhhh',\n",
       " 'ahmadinejad',\n",
       " 'ahmed',\n",
       " 'ahole',\n",
       " 'ahs',\n",
       " 'ai',\n",
       " 'aiadmk',\n",
       " 'aid',\n",
       " 'aidan',\n",
       " 'aide',\n",
       " 'aided',\n",
       " 'aiding',\n",
       " 'aig',\n",
       " 'ail',\n",
       " 'aila',\n",
       " 'ailing',\n",
       " 'aim',\n",
       " 'aiming',\n",
       " 'ainge',\n",
       " 'air',\n",
       " 'airbender',\n",
       " 'airbnb',\n",
       " 'airbnbs',\n",
       " 'airborne',\n",
       " 'airbus',\n",
       " 'aircraft',\n",
       " 'aire',\n",
       " 'airlifted',\n",
       " 'airline',\n",
       " 'airliner',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airshow',\n",
       " 'airspace',\n",
       " 'airstrike',\n",
       " 'airstrikes',\n",
       " 'airtanker',\n",
       " 'airwave',\n",
       " 'airway',\n",
       " 'aishwarya',\n",
       " 'aisle',\n",
       " 'aj',\n",
       " 'aka',\n",
       " 'akerman',\n",
       " 'akihabara',\n",
       " 'akkermans',\n",
       " 'akshay',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alabama',\n",
       " 'aladdin',\n",
       " 'alamo',\n",
       " 'alan',\n",
       " 'alanis',\n",
       " 'alarm',\n",
       " 'alarming',\n",
       " 'alarmingly',\n",
       " 'alaska',\n",
       " 'alaskan',\n",
       " 'alba',\n",
       " 'albanian',\n",
       " 'albany',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'alberto',\n",
       " 'albinism',\n",
       " 'albright',\n",
       " 'album',\n",
       " 'albus',\n",
       " 'alcatel',\n",
       " 'alcoa',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'alcoholrelated',\n",
       " 'aldean',\n",
       " 'aldulaimi',\n",
       " 'aleague',\n",
       " 'alert',\n",
       " 'alesis',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'alexandra',\n",
       " 'alexandria',\n",
       " 'alexandrina',\n",
       " 'alfred',\n",
       " 'algaddafi',\n",
       " 'algae',\n",
       " 'algarve',\n",
       " 'algebra',\n",
       " 'algeria',\n",
       " 'algerian',\n",
       " 'algorithmic',\n",
       " 'ali',\n",
       " 'alias',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alien',\n",
       " 'alinghi',\n",
       " 'aliph',\n",
       " 'alissa',\n",
       " 'alist',\n",
       " 'alistair',\n",
       " 'alitalia',\n",
       " 'alito',\n",
       " 'alive',\n",
       " 'aljazeera',\n",
       " 'allan',\n",
       " 'allegation',\n",
       " 'allege',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'alleges',\n",
       " 'allegiant',\n",
       " 'allen',\n",
       " 'allergy',\n",
       " 'allgirl',\n",
       " 'allgirls',\n",
       " 'alliance',\n",
       " 'allied',\n",
       " 'allinone',\n",
       " 'allison',\n",
       " 'allmale',\n",
       " 'allnatural',\n",
       " 'allnighter',\n",
       " 'allotting',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allstar',\n",
       " 'alltime',\n",
       " 'alludes',\n",
       " 'ally',\n",
       " 'almaliki',\n",
       " 'almegrahi',\n",
       " 'almond',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'alonso',\n",
       " 'alopecia',\n",
       " 'aloud',\n",
       " 'alp',\n",
       " 'alpha',\n",
       " 'alpine',\n",
       " 'alqaeda',\n",
       " 'alqaida',\n",
       " 'already',\n",
       " 'alsadr',\n",
       " 'alshabaab',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'alter',\n",
       " 'altercation',\n",
       " 'altered',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'althea',\n",
       " 'altitude',\n",
       " 'altmire',\n",
       " 'altona',\n",
       " 'alvaro',\n",
       " 'always',\n",
       " 'alzarqawis',\n",
       " 'alzawahiri',\n",
       " 'alzheimers',\n",
       " 'ama',\n",
       " 'amanda',\n",
       " 'amandla',\n",
       " 'amas',\n",
       " 'amateur',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'amazoncom',\n",
       " 'amb',\n",
       " 'ambassador',\n",
       " 'amber',\n",
       " 'ambition',\n",
       " 'ambivert',\n",
       " 'ambrose',\n",
       " 'ambulance',\n",
       " 'ambush',\n",
       " 'amc21',\n",
       " 'amd',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americanborn',\n",
       " 'americanstyle',\n",
       " 'ames',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'amigurumi',\n",
       " 'amin',\n",
       " 'ammo',\n",
       " 'amnesty',\n",
       " 'amoeba',\n",
       " 'amondson',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'amplifies',\n",
       " 'amputee',\n",
       " 'amstell',\n",
       " 'amsterdam',\n",
       " 'amtrak',\n",
       " 'amusement',\n",
       " 'amy',\n",
       " 'amys',\n",
       " 'amélie',\n",
       " 'an32',\n",
       " 'anaconda',\n",
       " 'anakin',\n",
       " 'analog',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analytical',\n",
       " 'analyze',\n",
       " 'analyzing',\n",
       " 'anarchist',\n",
       " 'anatomy',\n",
       " 'anc',\n",
       " 'ancestor',\n",
       " 'anchor',\n",
       " 'ancient',\n",
       " 'andalusian',\n",
       " 'andaman',\n",
       " 'anders',\n",
       " 'anderson',\n",
       " 'andes',\n",
       " 'andi',\n",
       " 'andreas',\n",
       " 'andrew',\n",
       " 'andrey',\n",
       " 'android',\n",
       " 'andromedon',\n",
       " 'andry',\n",
       " 'andy',\n",
       " 'anecdote',\n",
       " 'angara',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelesclass',\n",
       " 'angelica',\n",
       " 'angelina',\n",
       " 'angelo',\n",
       " 'anger',\n",
       " 'angered',\n",
       " 'angering',\n",
       " 'angle',\n",
       " 'anglia',\n",
       " 'anglican',\n",
       " 'anglo',\n",
       " 'anglofrench',\n",
       " 'angola',\n",
       " 'angolan',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'animal',\n",
       " 'animated',\n",
       " 'animator',\n",
       " 'anime',\n",
       " 'animorphs',\n",
       " 'aniston',\n",
       " 'anita',\n",
       " 'ankle',\n",
       " 'anna',\n",
       " 'annalise',\n",
       " 'annan',\n",
       " 'anne',\n",
       " 'annenberg',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'annonunced',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcer',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annoy',\n",
       " 'annoying',\n",
       " 'annual',\n",
       " 'annuls',\n",
       " 'anonymous',\n",
       " 'anonymously',\n",
       " 'anorexia',\n",
       " 'another',\n",
       " 'ansari',\n",
       " 'ansaris',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answerscom',\n",
       " 'ant',\n",
       " 'antananarivo',\n",
       " 'antarctic',\n",
       " 'antarctica',\n",
       " 'ante',\n",
       " 'antenna',\n",
       " 'anthem',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a20fc7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15379</th>\n",
       "      <th>15380</th>\n",
       "      <th>15381</th>\n",
       "      <th>15382</th>\n",
       "      <th>15383</th>\n",
       "      <th>15384</th>\n",
       "      <th>15385</th>\n",
       "      <th>15386</th>\n",
       "      <th>15387</th>\n",
       "      <th>15388</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "1      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "2      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "3      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "4      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "\n",
       "   15379  15380  15381  15382  15383  15384  15385  15386  15387  15388  \n",
       "0      0      0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 15389 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_df = pd.DataFrame(x_count.toarray())\n",
    "x_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63145afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count_df.columns = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed5a9f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>00s</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoombak</th>\n",
       "      <th>zotob</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zykina</th>\n",
       "      <th>ángel</th>\n",
       "      <th>íngrid</th>\n",
       "      <th>ürümqi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  00s  01  02  04  08  09  10  100  1000  ...  zoom  zoombak  zotob  \\\n",
       "0    0    0   0   0   0   0   0   0    0     0  ...     0        0      0   \n",
       "1    0    0   0   0   0   0   0   0    0     0  ...     0        0      0   \n",
       "2    0    0   0   0   0   0   0   0    0     0  ...     0        0      0   \n",
       "3    0    0   0   0   0   0   0   0    0     0  ...     0        0      0   \n",
       "4    0    0   0   0   0   0   0   0    0     0  ...     0        0      0   \n",
       "\n",
       "   zuckerberg  zuma  zurich  zykina  ángel  íngrid  ürümqi  \n",
       "0           0     0       0       0      0       0       0  \n",
       "1           0     0       0       0      0       0       0  \n",
       "2           0     0       0       0      0       0       0  \n",
       "3           0     0       0       0      0       0       0  \n",
       "4           0     0       0       0      0       0       0  \n",
       "\n",
       "[5 rows x 15389 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9338a",
   "metadata": {},
   "source": [
    "### Count Vectorizer (N-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c6708ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "      <th>headline_no_punctuation</th>\n",
       "      <th>headline_tokenized</th>\n",
       "      <th>headline_no_stopwords</th>\n",
       "      <th>headline_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>[should, i, get, bings]</td>\n",
       "      <td>[get, bings]</td>\n",
       "      <td>get bings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>[which, tv, female, friend, group, do, you, be...</td>\n",
       "      <td>[tv, female, friend, group, belong]</td>\n",
       "      <td>tv female friend group belong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "      <td>The New Star Wars The Force Awakens Trailer Is...</td>\n",
       "      <td>[the, new, star, wars, the, force, awakens, tr...</td>\n",
       "      <td>[new, star, wars, force, awakens, trailer, giv...</td>\n",
       "      <td>new star war force awakens trailer give chill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "      <td>This Vine Of New York On Celebrity Big Brother...</td>\n",
       "      <td>[this, vine, of, new, york, on, celebrity, big...</td>\n",
       "      <td>[vine, new, york, celebrity, big, brother, fuc...</td>\n",
       "      <td>vine new york celebrity big brother fucking pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>[a, couple, did, a, stunning, photo, shoot, wi...</td>\n",
       "      <td>[couple, stunning, photo, shoot, baby, learnin...</td>\n",
       "      <td>couple stunning photo shoot baby learning inop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait  \\\n",
       "0                                 Should I Get Bings          1   \n",
       "1      Which TV Female Friend Group Do You Belong In          1   \n",
       "2  The New \"Star Wars: The Force Awakens\" Trailer...          1   \n",
       "3  This Vine Of New York On \"Celebrity Big Brothe...          1   \n",
       "4  A Couple Did A Stunning Photo Shoot With Their...          1   \n",
       "\n",
       "                             headline_no_punctuation  \\\n",
       "0                                 Should I Get Bings   \n",
       "1      Which TV Female Friend Group Do You Belong In   \n",
       "2  The New Star Wars The Force Awakens Trailer Is...   \n",
       "3  This Vine Of New York On Celebrity Big Brother...   \n",
       "4  A Couple Did A Stunning Photo Shoot With Their...   \n",
       "\n",
       "                                  headline_tokenized  \\\n",
       "0                            [should, i, get, bings]   \n",
       "1  [which, tv, female, friend, group, do, you, be...   \n",
       "2  [the, new, star, wars, the, force, awakens, tr...   \n",
       "3  [this, vine, of, new, york, on, celebrity, big...   \n",
       "4  [a, couple, did, a, stunning, photo, shoot, wi...   \n",
       "\n",
       "                               headline_no_stopwords  \\\n",
       "0                                       [get, bings]   \n",
       "1                [tv, female, friend, group, belong]   \n",
       "2  [new, star, wars, force, awakens, trailer, giv...   \n",
       "3  [vine, new, york, celebrity, big, brother, fuc...   \n",
       "4  [couple, stunning, photo, shoot, baby, learnin...   \n",
       "\n",
       "                                 headline_lemmatized  \n",
       "0                                          get bings  \n",
       "1                      tv female friend group belong  \n",
       "2      new star war force awakens trailer give chill  \n",
       "3  vine new york celebrity big brother fucking pe...  \n",
       "4  couple stunning photo shoot baby learning inop...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2 = click_data.copy()\n",
    "data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a8dae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range is unigram, bigram, trigram and so on\n",
    "ngram_vect = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29885e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count_ngram = ngram_vect.fit_transform(data_2['headline_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ac9b26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 67687)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34a262d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000 run',\n",
       " '00s bad',\n",
       " '00s british',\n",
       " '00s disney',\n",
       " '00s everyone',\n",
       " '00s forgot',\n",
       " '00s girl',\n",
       " '00s hit',\n",
       " '00s kid',\n",
       " '00s movie',\n",
       " '00s music',\n",
       " '00s pop',\n",
       " '00s still',\n",
       " '00s teen',\n",
       " '00s youll',\n",
       " '01 percent',\n",
       " '02 last',\n",
       " '04 last',\n",
       " '08 matchday',\n",
       " '09 forssell',\n",
       " '09 goalkeeper',\n",
       " '09 inter',\n",
       " '09 wolfsburg',\n",
       " '10 15',\n",
       " '10 2008',\n",
       " '10 amazing',\n",
       " '10 arrested',\n",
       " '10 authority',\n",
       " '10 awesome',\n",
       " '10 awkward',\n",
       " '10 basic',\n",
       " '10 billionth',\n",
       " '10 bold',\n",
       " '10 celebs',\n",
       " '10 centimetre',\n",
       " '10 cheating',\n",
       " '10 christmas',\n",
       " '10 constellation',\n",
       " '10 day',\n",
       " '10 delicious',\n",
       " '10 excuse',\n",
       " '10 food',\n",
       " '10 friend',\n",
       " '10 ghost',\n",
       " '10 give',\n",
       " '10 gogo',\n",
       " '10 google',\n",
       " '10 hairstyle',\n",
       " '10 injured',\n",
       " '10 jackolanterns',\n",
       " '10 le',\n",
       " '10 leongatha',\n",
       " '10 lifechanging',\n",
       " '10 men',\n",
       " '10 mesmerizing',\n",
       " '10 million',\n",
       " '10 millionth',\n",
       " '10 minute',\n",
       " '10 month',\n",
       " '10 mouthwatering',\n",
       " '10 movie',\n",
       " '10 much',\n",
       " '10 mundane',\n",
       " '10 pair',\n",
       " '10 people',\n",
       " '10 perk',\n",
       " '10 popular',\n",
       " '10 question',\n",
       " '10 rare',\n",
       " '10 reason',\n",
       " '10 round',\n",
       " '10 second',\n",
       " '10 strangest',\n",
       " '10 surprisingly',\n",
       " '10 swoonworthy',\n",
       " '10 thing',\n",
       " '10 tip',\n",
       " '10 truth',\n",
       " '10 uefa',\n",
       " '10 way',\n",
       " '10 west',\n",
       " '10 world',\n",
       " '10 year',\n",
       " '100 10hour',\n",
       " '100 baghdad',\n",
       " '100 best',\n",
       " '100 better',\n",
       " '100 completely',\n",
       " '100 day',\n",
       " '100 dead',\n",
       " '100 die',\n",
       " '100 image',\n",
       " '100 killed',\n",
       " '100 love',\n",
       " '100 madoff',\n",
       " '100 make',\n",
       " '100 million',\n",
       " '100 mortgage',\n",
       " '100 moved',\n",
       " '100 need',\n",
       " '100 night',\n",
       " '100 note',\n",
       " '100 participate',\n",
       " '100 quiz',\n",
       " '100 season',\n",
       " '100 shiite',\n",
       " '100 soldier',\n",
       " '100 thousand',\n",
       " '100 tv',\n",
       " '100 year',\n",
       " '1000 arrested',\n",
       " '1000 month',\n",
       " '1000 store',\n",
       " '1000 win',\n",
       " '10000 2009',\n",
       " '10000 oil',\n",
       " '100000 haiti',\n",
       " '100000 parent',\n",
       " '100000 protest',\n",
       " '100000 video',\n",
       " '10000copy run',\n",
       " '10000th article',\n",
       " '100k prize',\n",
       " '100th season',\n",
       " '100yearold wedding',\n",
       " '100ºf heat',\n",
       " '101 dalmatian',\n",
       " '101 run',\n",
       " '103 game',\n",
       " '105 year',\n",
       " '106 billion',\n",
       " '106 star',\n",
       " '106 unexpectedly',\n",
       " '10600 yankee',\n",
       " '108 beautiful',\n",
       " '109 protest',\n",
       " '109th boston',\n",
       " '10hour workday',\n",
       " '10minute pizza',\n",
       " '10second clip',\n",
       " '10th birthday',\n",
       " '10th floor',\n",
       " '10year prison',\n",
       " '10yearold child',\n",
       " '10yearold spaniel',\n",
       " '11 000',\n",
       " '11 2008',\n",
       " '11 adorable',\n",
       " '11 art',\n",
       " '11 awkward',\n",
       " '11 beautiful',\n",
       " '11 beirut',\n",
       " '11 best',\n",
       " '11 billion',\n",
       " '11 bollywood',\n",
       " '11 bowl',\n",
       " '11 celebrity',\n",
       " '11 chart',\n",
       " '11 christmas',\n",
       " '11 classic',\n",
       " '11 craigslist',\n",
       " '11 cute',\n",
       " '11 deeply',\n",
       " '11 far',\n",
       " '11 fatshaming',\n",
       " '11 genderprogressive',\n",
       " '11 gift',\n",
       " '11 halloween',\n",
       " '11 haunted',\n",
       " '11 honest',\n",
       " '11 horrifyingly',\n",
       " '11 hugest',\n",
       " '11 iconic',\n",
       " '11 incredibly',\n",
       " '11 ingenious',\n",
       " '11 insanely',\n",
       " '11 internet',\n",
       " '11 kansa',\n",
       " '11 killed',\n",
       " '11 movie',\n",
       " '11 mp',\n",
       " '11 people',\n",
       " '11 photo',\n",
       " '11 pick',\n",
       " '11 planner',\n",
       " '11 premiere',\n",
       " '11 question',\n",
       " '11 quote',\n",
       " '11 real',\n",
       " '11 reason',\n",
       " '11 ridiculous',\n",
       " '11 scary',\n",
       " '11 scientific',\n",
       " '11 sex',\n",
       " '11 shameful',\n",
       " '11 shibas',\n",
       " '11 sign',\n",
       " '11 software',\n",
       " '11 song',\n",
       " '11 steamy',\n",
       " '11 struggle',\n",
       " '11 surprising',\n",
       " '11 tattoo',\n",
       " '11 thanksgiving',\n",
       " '11 thing',\n",
       " '11 time',\n",
       " '11 tragic',\n",
       " '11 turkey',\n",
       " '11 type',\n",
       " '11 underrated',\n",
       " '11 valentine',\n",
       " '11 vegan',\n",
       " '11 weird',\n",
       " '11 woman',\n",
       " '11 work',\n",
       " '1100 dealer',\n",
       " '1100 dealership',\n",
       " '1100 evacuated',\n",
       " '113 million',\n",
       " '113 reported',\n",
       " '117 billion',\n",
       " '119 billion',\n",
       " '11k tiny',\n",
       " '11th consecutive',\n",
       " '11th lacrosse',\n",
       " '11th suspect',\n",
       " '12 90',\n",
       " '12 amazing',\n",
       " '12 animal',\n",
       " '12 awkward',\n",
       " '12 burning',\n",
       " '12 celebrity',\n",
       " '12 chart',\n",
       " '12 chili',\n",
       " '12 classic',\n",
       " '12 crazy',\n",
       " '12 decadent',\n",
       " '12 disneyland',\n",
       " '12 diva',\n",
       " '12 dumb',\n",
       " '12 easy',\n",
       " '12 emojis',\n",
       " '12 filthy',\n",
       " '12 finish',\n",
       " '12 firefighter',\n",
       " '12 functioning',\n",
       " '12 georgetown',\n",
       " '12 gift',\n",
       " '12 glorious',\n",
       " '12 grim',\n",
       " '12 hack',\n",
       " '12 hilarious',\n",
       " '12 horrifyingly',\n",
       " '12 hour',\n",
       " '12 iconic',\n",
       " '12 incredible',\n",
       " '12 india',\n",
       " '12 indian',\n",
       " '12 injures',\n",
       " '12 insanely',\n",
       " '12 inspiring',\n",
       " '12 jimmy',\n",
       " '12 life',\n",
       " '12 lowcarb',\n",
       " '12 moment',\n",
       " '12 month',\n",
       " '12 motivational',\n",
       " '12 notre',\n",
       " '12 oversized',\n",
       " '12 painfully',\n",
       " '12 penalty',\n",
       " '12 people',\n",
       " '12 photo',\n",
       " '12 pick',\n",
       " '12 picture',\n",
       " '12 place',\n",
       " '12 powerful',\n",
       " '12 pregnancy',\n",
       " '12 presidential',\n",
       " '12 ps1',\n",
       " '12 punch',\n",
       " '12 quarter',\n",
       " '12 reason',\n",
       " '12 recipe',\n",
       " '12 relatable',\n",
       " '12 sassiest',\n",
       " '12 scientist',\n",
       " '12 semifinal',\n",
       " '12 sign',\n",
       " '12 struggle',\n",
       " '12 stunning',\n",
       " '12 terrifying',\n",
       " '12 thing',\n",
       " '12 time',\n",
       " '12 title',\n",
       " '12 tournament',\n",
       " '12 tree',\n",
       " '12 undeniable',\n",
       " '12 unexpectedly',\n",
       " '12 way',\n",
       " '12 weird',\n",
       " '12 wildly',\n",
       " '12 year',\n",
       " '1200 school',\n",
       " '12000 penny',\n",
       " '1200mile range',\n",
       " '121 day',\n",
       " '1215 december',\n",
       " '126292pound truck',\n",
       " '12th doctor',\n",
       " '12yearold brother',\n",
       " '12yearold school',\n",
       " '12yearold sikh',\n",
       " '13 accurate',\n",
       " '13 amazingly',\n",
       " '13 animal',\n",
       " '13 annoying',\n",
       " '13 apps',\n",
       " '13 art',\n",
       " '13 award',\n",
       " '13 awkward',\n",
       " '13 backyard',\n",
       " '13 basic',\n",
       " '13 beautiful',\n",
       " '13 best',\n",
       " '13 big',\n",
       " '13 billion',\n",
       " '13 body',\n",
       " '13 brutally',\n",
       " '13 celebrity',\n",
       " '13 celebs',\n",
       " '13 chart',\n",
       " '13 confession',\n",
       " '13 cute',\n",
       " '13 cuties',\n",
       " '13 dad',\n",
       " '13 dead',\n",
       " '13 death',\n",
       " '13 demolished',\n",
       " '13 disturbing',\n",
       " '13 drugresistant',\n",
       " '13 eyeopening',\n",
       " '13 fact',\n",
       " '13 film',\n",
       " '13 firm',\n",
       " '13 german',\n",
       " '13 gifs',\n",
       " '13 gift',\n",
       " '13 glorious',\n",
       " '13 haiku',\n",
       " '13 halloween',\n",
       " '13 hanukkah',\n",
       " '13 harry',\n",
       " '13 hashtags',\n",
       " '13 hilarious',\n",
       " '13 holiday',\n",
       " '13 honest',\n",
       " '13 horror',\n",
       " '13 hour',\n",
       " '13 idea',\n",
       " '13 indian',\n",
       " '13 insanely',\n",
       " '13 killed',\n",
       " '13 lesbian',\n",
       " '13 lie',\n",
       " '13 lyric',\n",
       " '13 magnificent',\n",
       " '13 marvel',\n",
       " '13 meetcute',\n",
       " '13 men',\n",
       " '13 million',\n",
       " '13 misogynistic',\n",
       " '13 missing',\n",
       " '13 moment',\n",
       " '13 movie',\n",
       " '13 new',\n",
       " '13 nobake',\n",
       " '13 part',\n",
       " '13 pedro',\n",
       " '13 pick',\n",
       " '13 picture',\n",
       " '13 question',\n",
       " '13 reason',\n",
       " '13 ridiculously',\n",
       " '13 sign',\n",
       " '13 smoking',\n",
       " '13 spellbooks',\n",
       " '13 stage',\n",
       " '13 state',\n",
       " '13 struggle',\n",
       " '13 terrible',\n",
       " '13 terrifying',\n",
       " '13 text',\n",
       " '13 thing',\n",
       " '13 time',\n",
       " '13 traralgon',\n",
       " '13 tumblr',\n",
       " '13 tv',\n",
       " '13 unbelievably',\n",
       " '13 way',\n",
       " '13 weird',\n",
       " '13 win',\n",
       " '13 wise',\n",
       " '13 woman',\n",
       " '13 world',\n",
       " '13 year',\n",
       " '13 young',\n",
       " '130 dead',\n",
       " '13000 people',\n",
       " '133 trillion',\n",
       " '134 filipino',\n",
       " '13500 evacuated',\n",
       " '136 billion',\n",
       " '137 foot',\n",
       " '13yearold kid',\n",
       " '14 adorable',\n",
       " '14 awkward',\n",
       " '14 ball',\n",
       " '14 beautiful',\n",
       " '14 beautifully',\n",
       " '14 bed',\n",
       " '14 best',\n",
       " '14 billion',\n",
       " '14 celebrity',\n",
       " '14 chart',\n",
       " '14 colorado',\n",
       " '14 confession',\n",
       " '14 couple',\n",
       " '14 crazy',\n",
       " '14 crucial',\n",
       " '14 deliciously',\n",
       " '14 detention',\n",
       " '14 disgustingly',\n",
       " '14 diy',\n",
       " '14 dog',\n",
       " '14 duet',\n",
       " '14 facebook',\n",
       " '14 fall',\n",
       " '14 flawless',\n",
       " '14 genius',\n",
       " '14 gift',\n",
       " '14 glutenfree',\n",
       " '14 graph',\n",
       " '14 heartbreaking',\n",
       " '14 honest',\n",
       " '14 horror',\n",
       " '14 important',\n",
       " '14 incredible',\n",
       " '14 insanely',\n",
       " '14 irresistible',\n",
       " '14 killed',\n",
       " '14 men',\n",
       " '14 mma',\n",
       " '14 model',\n",
       " '14 new',\n",
       " '14 pair',\n",
       " '14 people',\n",
       " '14 perfect',\n",
       " '14 photo',\n",
       " '14 picture',\n",
       " '14 powerful',\n",
       " '14 quarter',\n",
       " '14 rare',\n",
       " '14 reason',\n",
       " '14 ridiculously',\n",
       " '14 scrumptious',\n",
       " '14 soldier',\n",
       " '14 star',\n",
       " '14 struggle',\n",
       " '14 supermarket',\n",
       " '14 surprising',\n",
       " '14 text',\n",
       " '14 thing',\n",
       " '14 time',\n",
       " '14 trend',\n",
       " '14 type',\n",
       " '14 unbelievably',\n",
       " '14 way',\n",
       " '14 woman',\n",
       " '140 billion',\n",
       " '14000acre southern',\n",
       " '1419 september',\n",
       " '144 percent',\n",
       " '146 billion',\n",
       " '147 gang',\n",
       " '149 alqaeda',\n",
       " '14yearold fan',\n",
       " '14yearold found',\n",
       " '15 2008',\n",
       " '15 absolutely',\n",
       " '15 aim',\n",
       " '15 animal',\n",
       " '15 appetizer',\n",
       " '15 autumnthemed',\n",
       " '15 average',\n",
       " '15 awesome',\n",
       " '15 beautiful',\n",
       " '15 best',\n",
       " '15 billion',\n",
       " '15 blasphemous',\n",
       " '15 body',\n",
       " '15 boozy',\n",
       " '15 bowl',\n",
       " '15 brilliant',\n",
       " '15 bumper',\n",
       " '15 celebrity',\n",
       " '15 chart',\n",
       " '15 cheap',\n",
       " '15 christian',\n",
       " '15 christmas',\n",
       " '15 confession',\n",
       " '15 confirmed',\n",
       " '15 cooky',\n",
       " '15 cringeworthy',\n",
       " '15 crucial',\n",
       " '15 delectable',\n",
       " '15 delicious',\n",
       " '15 deliciously',\n",
       " '15 depressing',\n",
       " '15 die',\n",
       " '15 dog',\n",
       " '15 dollar',\n",
       " '15 duet',\n",
       " '15 easy',\n",
       " '15 expensive',\n",
       " '15 exquisite',\n",
       " '15 fifty',\n",
       " '15 fresher',\n",
       " '15 gamechanging',\n",
       " '15 gary',\n",
       " '15 geordie',\n",
       " '15 gift',\n",
       " '15 ginger',\n",
       " '15 gorgeous',\n",
       " '15 great',\n",
       " '15 halloween',\n",
       " '15 hanukkah',\n",
       " '15 highly',\n",
       " '15 hilarious',\n",
       " '15 hit',\n",
       " '15 holiday',\n",
       " '15 home',\n",
       " '15 honest',\n",
       " '15 hot',\n",
       " '15 humorously',\n",
       " '15 incredibly',\n",
       " '15 indian',\n",
       " '15 injured',\n",
       " '15 insane',\n",
       " '15 insanely',\n",
       " '15 inspiring',\n",
       " '15 instrumental',\n",
       " '15 invisible',\n",
       " '15 iraq',\n",
       " '15 item',\n",
       " '15 lazy',\n",
       " '15 life',\n",
       " '15 lifechanging',\n",
       " '15 lightly',\n",
       " '15 men',\n",
       " '15 million',\n",
       " '15 mindblowing',\n",
       " '15 moment',\n",
       " '15 mountain',\n",
       " '15 mouthwatering',\n",
       " '15 movie',\n",
       " '15 music',\n",
       " '15 nanometre',\n",
       " '15 new',\n",
       " '15 nyc',\n",
       " '15 obnoxious',\n",
       " '15 ooey',\n",
       " '15 penis',\n",
       " '15 people',\n",
       " '15 percent',\n",
       " '15 perfect',\n",
       " '15 pet',\n",
       " '15 phrase',\n",
       " '15 picture',\n",
       " '15 place',\n",
       " '15 portrait',\n",
       " '15 powerful',\n",
       " '15 purrfect',\n",
       " '15 question',\n",
       " '15 rad',\n",
       " '15 random',\n",
       " '15 really',\n",
       " '15 reason',\n",
       " '15 resolution',\n",
       " '15 ridiculously',\n",
       " '15 scrumptious',\n",
       " '15 season',\n",
       " '15 secret',\n",
       " '15 set',\n",
       " '15 sickeningly',\n",
       " '15 sign',\n",
       " '15 situation',\n",
       " '15 somalia',\n",
       " '15 song',\n",
       " '15 southwest',\n",
       " '15 state',\n",
       " '15 struggle',\n",
       " '15 stunning',\n",
       " '15 stylishly',\n",
       " '15 super',\n",
       " '15 surprising',\n",
       " '15 tee',\n",
       " '15 terrifying',\n",
       " '15 text',\n",
       " '15 thing',\n",
       " '15 thought',\n",
       " '15 time',\n",
       " '15 trivia',\n",
       " '15 tumblr',\n",
       " '15 turkey',\n",
       " '15 tv',\n",
       " '15 unbearable',\n",
       " '15 useful',\n",
       " '15 video',\n",
       " '15 vine',\n",
       " '15 vintage',\n",
       " '15 way',\n",
       " '15 weird',\n",
       " '15 winter',\n",
       " '15 woman',\n",
       " '15 work',\n",
       " '15 world',\n",
       " '15 year',\n",
       " '150 bridge',\n",
       " '150 friend',\n",
       " '150 metre',\n",
       " '150 mile',\n",
       " '150 million',\n",
       " '150 year',\n",
       " '15000 attended',\n",
       " '15000 dangerous',\n",
       " '15000 job',\n",
       " '153 aboard',\n",
       " '153 billion',\n",
       " '153 people',\n",
       " '153000 ransom',\n",
       " '159 win',\n",
       " '15m apply',\n",
       " '15second video',\n",
       " '15yearold girl',\n",
       " '16 18story',\n",
       " '16 2016',\n",
       " '16 adorable',\n",
       " '16 adult',\n",
       " '16 animal',\n",
       " '16 baghdad',\n",
       " '16 beautifully',\n",
       " '16 billion',\n",
       " '16 bird',\n",
       " '16 black',\n",
       " '16 boozy',\n",
       " '16 calming',\n",
       " '16 celebrity',\n",
       " '16 chart',\n",
       " '16 chocolate',\n",
       " '16 clapbacks',\n",
       " '16 confession',\n",
       " '16 cosplayers',\n",
       " '16 craziest',\n",
       " '16 cute',\n",
       " '16 delicious',\n",
       " '16 delightful',\n",
       " '16 exciting',\n",
       " '16 exit',\n",
       " '16 extraordinary',\n",
       " '16 eyebrow',\n",
       " '16 fact',\n",
       " '16 fairy',\n",
       " '16 field',\n",
       " '16 foot',\n",
       " '16 gorgeous',\n",
       " '16 hilarious',\n",
       " '16 honest',\n",
       " '16 imaginative',\n",
       " '16 immigrant',\n",
       " '16 important',\n",
       " '16 incredible',\n",
       " '16 iphone',\n",
       " '16 jackie',\n",
       " '16 justin',\n",
       " '16 killer',\n",
       " '16 london',\n",
       " '16 luscious',\n",
       " '16 mac',\n",
       " '16 made',\n",
       " '16 moment',\n",
       " '16 musician',\n",
       " '16 nerdily',\n",
       " '16 new',\n",
       " '16 nostalgic',\n",
       " '16 painfully',\n",
       " '16 people',\n",
       " '16 perfect',\n",
       " '16 pet',\n",
       " '16 photo',\n",
       " '16 picture',\n",
       " '16 puppy',\n",
       " '16 random',\n",
       " '16 reallife',\n",
       " '16 reason',\n",
       " '16 relationship',\n",
       " '16 scary',\n",
       " '16 shittiest',\n",
       " '16 sibling',\n",
       " '16 sign',\n",
       " '16 simpson',\n",
       " '16 snapchats',\n",
       " '16 spanish',\n",
       " '16 split',\n",
       " '16 teen',\n",
       " '16 terrible',\n",
       " '16 text',\n",
       " '16 thing',\n",
       " '16 time',\n",
       " '16 truly',\n",
       " '16 tweet',\n",
       " '16 unwritten',\n",
       " '16 way',\n",
       " '16 weird',\n",
       " '16 weirdest',\n",
       " '16 wikipedia',\n",
       " '16 word',\n",
       " '16 worst',\n",
       " '16 young',\n",
       " '160 crash',\n",
       " '160 presumed',\n",
       " '163 dead',\n",
       " '164 billion',\n",
       " '164 million',\n",
       " '165 mph',\n",
       " '168 rob',\n",
       " '16death crash',\n",
       " '16hour loud',\n",
       " '16th century',\n",
       " '16year low',\n",
       " '16yearold brother',\n",
       " '16yearold camera',\n",
       " '16yearold self',\n",
       " '17 2009',\n",
       " '17 adorable',\n",
       " '17 amazing',\n",
       " '17 amazon',\n",
       " '17 award',\n",
       " '17 babysitting',\n",
       " '17 badass',\n",
       " '17 beautiful',\n",
       " '17 beautifully',\n",
       " '17 beauty',\n",
       " '17 beaver',\n",
       " '17 best',\n",
       " '17 billion',\n",
       " '17 board',\n",
       " '17 bold',\n",
       " '17 book',\n",
       " '17 boozy',\n",
       " '17 branch',\n",
       " '17 brilliant',\n",
       " '17 cat',\n",
       " '17 celebrity',\n",
       " '17 celebs',\n",
       " '17 cheesy',\n",
       " '17 city',\n",
       " '17 clever',\n",
       " '17 colorado',\n",
       " '17 compelling',\n",
       " '17 completely',\n",
       " '17 confession',\n",
       " '17 cosplayers',\n",
       " '17 creepy',\n",
       " '17 dad',\n",
       " '17 deeply',\n",
       " '17 delectable',\n",
       " '17 delicious',\n",
       " '17 dessert',\n",
       " '17 disney',\n",
       " '17 dog',\n",
       " '17 donald',\n",
       " '17 drag',\n",
       " '17 dramatic',\n",
       " '17 easy',\n",
       " '17 embarrassing',\n",
       " '17 emotion',\n",
       " '17 entirely',\n",
       " '17 eyeopening',\n",
       " '17 face',\n",
       " '17 facebook',\n",
       " '17 fact',\n",
       " '17 fall',\n",
       " '17 female',\n",
       " '17 first',\n",
       " '17 food',\n",
       " '17 funniest',\n",
       " '17 genius',\n",
       " '17 gift',\n",
       " '17 glitch',\n",
       " '17 gorgeous',\n",
       " '17 great',\n",
       " '17 greatest',\n",
       " '17 hairdresser',\n",
       " '17 halloween',\n",
       " '17 healthier',\n",
       " '17 heartbreaking',\n",
       " '17 heartshaped',\n",
       " '17 hilarious',\n",
       " '17 holiday',\n",
       " '17 home',\n",
       " '17 horrible',\n",
       " '17 horribly',\n",
       " '17 house',\n",
       " '17 iconic',\n",
       " '17 image',\n",
       " '17 important',\n",
       " '17 impossibly',\n",
       " '17 inappropriate',\n",
       " '17 incredible',\n",
       " '17 incredibly',\n",
       " '17 inexplicably',\n",
       " '17 insanely',\n",
       " '17 inspirational',\n",
       " '17 instagram',\n",
       " '17 itty',\n",
       " '17 joke',\n",
       " '17 killed',\n",
       " '17 kitchen',\n",
       " '17 knockoff',\n",
       " '17 know',\n",
       " '17 lifechanging',\n",
       " '17 lohanthony',\n",
       " '17 lowsugar',\n",
       " '17 makeup',\n",
       " '17 mcdonalds',\n",
       " '17 medieval',\n",
       " '17 meet',\n",
       " '17 meme',\n",
       " '17 mindblowingly',\n",
       " '17 minimalist',\n",
       " '17 misconception',\n",
       " '17 moment',\n",
       " '17 mouthwatering',\n",
       " '17 nude',\n",
       " '17 outrageosly',\n",
       " '17 outrageously',\n",
       " '17 overtly',\n",
       " '17 painful',\n",
       " '17 pair',\n",
       " '17 paleo',\n",
       " '17 pasta',\n",
       " '17 people',\n",
       " '17 perfect',\n",
       " '17 photo',\n",
       " '17 pick',\n",
       " '17 picture',\n",
       " '17 piece',\n",
       " '17 place',\n",
       " '17 popular',\n",
       " '17 powerful',\n",
       " '17 practical',\n",
       " '17 princess',\n",
       " '17 problem',\n",
       " '17 product',\n",
       " '17 productivity',\n",
       " '17 question',\n",
       " '17 reaction',\n",
       " '17 real',\n",
       " '17 really',\n",
       " '17 reason',\n",
       " '17 red',\n",
       " '17 relationship',\n",
       " '17 reyinspired',\n",
       " '17 ridiculous',\n",
       " '17 scandinavian',\n",
       " '17 secret',\n",
       " '17 secretly',\n",
       " '17 seemingly',\n",
       " '17 severely',\n",
       " '17 shania',\n",
       " '17 shocking',\n",
       " '17 sibling',\n",
       " '17 sign',\n",
       " '17 situation',\n",
       " '17 song',\n",
       " '17 strikingly',\n",
       " '17 struggle',\n",
       " '17 stunning',\n",
       " '17 stupidly',\n",
       " '17 subtle',\n",
       " '17 supernecessary',\n",
       " '17 tattoo',\n",
       " '17 text',\n",
       " '17 thanksgiving',\n",
       " '17 thing',\n",
       " '17 thought',\n",
       " '17 time',\n",
       " '17 tinder',\n",
       " '17 totally',\n",
       " '17 troop',\n",
       " '17 truth',\n",
       " '17 tumblr',\n",
       " '17 tweet',\n",
       " '17 tweeter',\n",
       " '17 twentysomething',\n",
       " '17 type',\n",
       " '17 underrated',\n",
       " '17 unforgettable',\n",
       " '17 unusual',\n",
       " '17 video',\n",
       " '17 way',\n",
       " '17 website',\n",
       " '17 wicked',\n",
       " '17 woman',\n",
       " '17 wonderful',\n",
       " '17 word',\n",
       " '17 worst',\n",
       " '17 young',\n",
       " '1700 job',\n",
       " '1700 runner',\n",
       " '1700 woman',\n",
       " '172 mph',\n",
       " '175 billion',\n",
       " '175 killed',\n",
       " '175 trillion',\n",
       " '17point play',\n",
       " '17th turkish',\n",
       " '17year peak',\n",
       " '17yearold artist',\n",
       " '18 ad',\n",
       " '18 adult',\n",
       " '18 american',\n",
       " '18 animal',\n",
       " '18 badass',\n",
       " '18 billion',\n",
       " '18 board',\n",
       " '18 booty',\n",
       " '18 breathtakingly',\n",
       " '18 brutal',\n",
       " '18 cat',\n",
       " '18 celebrity',\n",
       " '18 cheese',\n",
       " '18 confession',\n",
       " '18 cosy',\n",
       " '18 couple',\n",
       " '18 cringeworthy',\n",
       " '18 dating',\n",
       " '18 decadent',\n",
       " '18 deliciously',\n",
       " '18 difference',\n",
       " '18 dog',\n",
       " '18 dreamy',\n",
       " '18 easy',\n",
       " '18 eyepopping',\n",
       " '18 fabulous',\n",
       " '18 fact',\n",
       " '18 fall',\n",
       " '18 filipino',\n",
       " '18 fix',\n",
       " '18 food',\n",
       " '18 genius',\n",
       " '18 ghostly',\n",
       " '18 gift',\n",
       " '18 gloriously',\n",
       " '18 gorgeous',\n",
       " '18 greatest',\n",
       " '18 grilled',\n",
       " '18 hangover',\n",
       " '18 hilarious',\n",
       " '18 iconic',\n",
       " '18 illegal',\n",
       " '18 important',\n",
       " '18 incredibly',\n",
       " '18 injured',\n",
       " '18 insanely',\n",
       " '18 line',\n",
       " '18 lolworthy',\n",
       " '18 map',\n",
       " '18 million',\n",
       " '18 moving',\n",
       " '18 musthave',\n",
       " '18 offensive',\n",
       " '18 old',\n",
       " '18 parent',\n",
       " '18 people',\n",
       " '18 perfect',\n",
       " '18 photo',\n",
       " '18 pia',\n",
       " '18 pic',\n",
       " '18 picture',\n",
       " '18 place',\n",
       " '18 poppunk',\n",
       " '18 post',\n",
       " '18 powerful',\n",
       " '18 problem',\n",
       " '18 product',\n",
       " '18 question',\n",
       " '18 quick',\n",
       " '18 really',\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9c4ac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000 run</th>\n",
       "      <th>00s bad</th>\n",
       "      <th>00s british</th>\n",
       "      <th>00s disney</th>\n",
       "      <th>00s everyone</th>\n",
       "      <th>00s forgot</th>\n",
       "      <th>00s girl</th>\n",
       "      <th>00s hit</th>\n",
       "      <th>00s kid</th>\n",
       "      <th>00s movie</th>\n",
       "      <th>...</th>\n",
       "      <th>zuckerberg answered</th>\n",
       "      <th>zuckerberg victim</th>\n",
       "      <th>zuma charged</th>\n",
       "      <th>zuma chooses</th>\n",
       "      <th>zurich call</th>\n",
       "      <th>zurich switzerland</th>\n",
       "      <th>zykina dy</th>\n",
       "      <th>ángel cabrera</th>\n",
       "      <th>íngrid betancourt</th>\n",
       "      <th>ürümqi china</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67687 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000 run  00s bad  00s british  00s disney  00s everyone  00s forgot  \\\n",
       "0        0        0            0           0             0           0   \n",
       "1        0        0            0           0             0           0   \n",
       "2        0        0            0           0             0           0   \n",
       "3        0        0            0           0             0           0   \n",
       "4        0        0            0           0             0           0   \n",
       "\n",
       "   00s girl  00s hit  00s kid  00s movie  ...  zuckerberg answered  \\\n",
       "0         0        0        0          0  ...                    0   \n",
       "1         0        0        0          0  ...                    0   \n",
       "2         0        0        0          0  ...                    0   \n",
       "3         0        0        0          0  ...                    0   \n",
       "4         0        0        0          0  ...                    0   \n",
       "\n",
       "   zuckerberg victim  zuma charged  zuma chooses  zurich call  \\\n",
       "0                  0             0             0            0   \n",
       "1                  0             0             0            0   \n",
       "2                  0             0             0            0   \n",
       "3                  0             0             0            0   \n",
       "4                  0             0             0            0   \n",
       "\n",
       "   zurich switzerland  zykina dy  ángel cabrera  íngrid betancourt  \\\n",
       "0                   0          0              0                  0   \n",
       "1                   0          0              0                  0   \n",
       "2                   0          0              0                  0   \n",
       "3                   0          0              0                  0   \n",
       "4                   0          0              0                  0   \n",
       "\n",
       "   ürümqi china  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "\n",
       "[5 rows x 67687 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_ngram_df = pd.DataFrame(x_count_ngram.toarray(), columns = ngram_vect.get_feature_names())\n",
    "x_count_ngram_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6377d1",
   "metadata": {},
   "source": [
    "### Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "921fbcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35061c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = click_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f4c3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf__vect = TfidfVectorizer()\n",
    "x_tfidf = tfidf__vect.fit_transform(data_3['headline_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3de4fae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 15389)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f5ec7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000       0\n",
       "00s       0\n",
       "01        0\n",
       "02        0\n",
       "04        0\n",
       "         ..\n",
       "zurich    0\n",
       "zykina    0\n",
       "ángel     0\n",
       "íngrid    0\n",
       "ürümqi    0\n",
       "Length: 15389, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tfidf_df = pd.DataFrame(x_tfidf.toarray(), columns = tfidf__vect.get_feature_names())\n",
    "x_tfidf_df = x_tfidf_df.reset_index(drop=True)\n",
    "x_tfidf_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f3eda",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c7981",
   "metadata": {},
   "source": [
    "## <center>Feature Engineering</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2cd5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_4 = click_data.iloc[:, :2]\n",
    "#data_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb6012",
   "metadata": {},
   "source": [
    "### Counting the length of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f5192ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000                0\n",
       "00s                0\n",
       "01                 0\n",
       "02                 0\n",
       "04                 0\n",
       "                  ..\n",
       "zykina             0\n",
       "ángel              0\n",
       "íngrid             0\n",
       "ürümqi             0\n",
       "headline_length    0\n",
       "Length: 15390, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This feature is used to check if the length of clickbait headline is longer than that of normal headline\n",
    "#data_4['headline_length'] = data_4['headline'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "#data_4.head()\n",
    "x_tfidf_df['headline_length'] = click_data['headline'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "x_tfidf_df = x_tfidf_df.reset_index(drop=True)\n",
    "x_tfidf_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8250b3",
   "metadata": {},
   "source": [
    "### Calcultaing the percentage of the punctuation in the headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d96c7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Columns: 15391 entries, 000 to punctuation%\n",
      "dtypes: float64(15390), int64(1)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "def count_punctuation(text):\n",
    "    count = sum(1 for char in text if char in string.punctuation)\n",
    "    percent = np.round(count / (len(text) - text.count(\" \")),2) * 100\n",
    "    return percent\n",
    "\n",
    "#data_4['punctuation%'] = data_4['headline'].apply(lambda x: count_punctuation(x))\n",
    "#data_4.head()\n",
    "x_tfidf_df['punctuation%'] = click_data['headline'].apply(lambda x: count_punctuation(x))\n",
    "x_tfidf_df = x_tfidf_df.reset_index(drop=True)\n",
    "x_tfidf_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094dc2e",
   "metadata": {},
   "source": [
    "### Counting the percent of uppercase letters in the headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe169278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000                0\n",
       "00s                0\n",
       "01                 0\n",
       "02                 0\n",
       "04                 0\n",
       "                  ..\n",
       "íngrid             0\n",
       "ürümqi             0\n",
       "headline_length    0\n",
       "punctuation%       0\n",
       "uppercase%         0\n",
       "Length: 15392, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_uppercase_char(text):\n",
    "    count_uppercase = sum(1 for char in text if char.isupper())\n",
    "    percent_uppercase = np.round(count_uppercase / (len(text) - text.count(\" \")),2) * 100\n",
    "    return percent_uppercase\n",
    "\n",
    "#data_4['Uppercase%'] = data_4['headline'].apply(lambda x: count_uppercase_char(x))\n",
    "#data_4.head()\n",
    "x_tfidf_df['uppercase%'] = click_data['headline'].apply(lambda x: count_uppercase_char(x))\n",
    "x_tfidf_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418a2cf",
   "metadata": {},
   "source": [
    "### Count the number of question marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5bd5592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000                     0.0\n",
       "00s                     0.0\n",
       "01                      0.0\n",
       "02                      0.0\n",
       "04                      0.0\n",
       "                       ... \n",
       "ürümqi                  0.0\n",
       "headline_length        46.0\n",
       "punctuation%            0.0\n",
       "uppercase%              4.0\n",
       "question_mark_count     0.0\n",
       "Name: 13758, Length: 15393, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_question_mark(text):\n",
    "    count = sum(1 for char in text if char == \"?\")\n",
    "    return count\n",
    "\n",
    "#data_4['Question_mark_count'] = data_4['headline'].apply(lambda x: count_question_mark(x))\n",
    "#data_4.iloc[14023]\n",
    "x_tfidf_df['question_mark_count'] = click_data['headline'].apply(lambda x: count_question_mark(x))\n",
    "x_tfidf_df.iloc[13758]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d3851",
   "metadata": {},
   "source": [
    "### Evaluating the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6da2fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the headline length\n",
    "#bins = np.linspace(0, 120, 40)\n",
    "#plt.hist(data_4[data_4['clickbait']==1]['headline_length'], bins, alpha = 0.5, label = 'Clickbait')\n",
    "#plt.hist(data_4[data_4['clickbait']==0]['headline_length'], bins, alpha = 0.5, label = 'Not a Clickbait')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83449d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the headline punctuation percent\n",
    "#bins = np.linspace(0, 20, 40)\n",
    "#plt.hist(data_4[data_4['clickbait']==1]['punctuation%'], bins, alpha = 0.5, label = 'Clickbait')\n",
    "#plt.hist(data_4[data_4['clickbait']==0]['punctuation%'], bins, alpha = 0.5, label = 'Not a Clickbait')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "326b0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the headline uppercase percent\n",
    "#bins = np.linspace(0, 50, 40)\n",
    "#plt.hist(data_4[data_4['clickbait']==1]['Uppercase%'], bins, alpha = 0.5, label = 'Clickbait')\n",
    "#plt.hist(data_4[data_4['clickbait']==0]['Uppercase%'], bins, alpha = 0.5, label = 'Not a Clickbait')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5097f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the headline question mark count\n",
    "#bins = np.linspace(0, 5, 40)\n",
    "#plt.hist(data_4[data_4['clickbait']==1]['Question_mark_count'], bins, alpha = 0.5, label = 'Clickbait')\n",
    "#plt.hist(data_4[data_4['clickbait']==0]['Question_mark_count'], bins, alpha = 0.5, label = 'Not a Clickbait')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd96a8",
   "metadata": {},
   "source": [
    "### Box-Cox power transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e2e739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1, 6):\n",
    "#    plt.hist((data_4['punctuation%'])**(1/i), bins = 40)\n",
    "#    plt.title(\"Transformation: 1/{}\".format(str(i)))\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871912a",
   "metadata": {},
   "source": [
    "## <center>Random Forest Classifier through Cross-validation</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee23dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = x_tfidf_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28e56ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>00s</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>...</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zykina</th>\n",
       "      <th>ángel</th>\n",
       "      <th>íngrid</th>\n",
       "      <th>ürümqi</th>\n",
       "      <th>headline_length</th>\n",
       "      <th>punctuation%</th>\n",
       "      <th>uppercase%</th>\n",
       "      <th>question_mark_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15393 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  00s   01   02   04   08   09   10  100  1000  ...  zuma  zurich  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "\n",
       "   zykina  ángel  íngrid  ürümqi  headline_length  punctuation%  uppercase%  \\\n",
       "0     0.0    0.0     0.0     0.0               15           0.0        27.0   \n",
       "1     0.0    0.0     0.0     0.0               37           0.0        27.0   \n",
       "2     0.0    0.0     0.0     0.0               60           5.0        23.0   \n",
       "3     0.0    0.0     0.0     0.0               56           4.0        21.0   \n",
       "4     0.0    0.0     0.0     0.0               83           0.0        22.0   \n",
       "\n",
       "   question_mark_count  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "\n",
       "[5 rows x 15393 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4778b4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000                    0\n",
       "00s                    0\n",
       "01                     0\n",
       "02                     0\n",
       "04                     0\n",
       "                      ..\n",
       "ürümqi                 0\n",
       "headline_length        0\n",
       "punctuation%           0\n",
       "uppercase%             0\n",
       "question_mark_count    0\n",
       "Length: 15393, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6813a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_df\n",
    "target = click_data['clickbait']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8099efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import KFold, cross_val_score\n",
    "#from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "908d97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_jobs is set to -1 so that multiple jobs can be executed in parallel\n",
    "#rf = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "# KFold is used to divide the dataset into subsets such that it can divide the dataset into specified number of sets.\n",
    "# The model is then trained on k-1 sets and 1 set is kept aside as a test set\n",
    "#k_fold = KFold(n_splits=5)\n",
    "\n",
    "#cross_val_score(rf, features, target, cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd814c",
   "metadata": {},
   "source": [
    "## <center>Random Forest Classifier through holdout test set</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0a18d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "19ad9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b993a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "577c2cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>00s</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>...</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zykina</th>\n",
       "      <th>ángel</th>\n",
       "      <th>íngrid</th>\n",
       "      <th>ürümqi</th>\n",
       "      <th>headline_length</th>\n",
       "      <th>punctuation%</th>\n",
       "      <th>uppercase%</th>\n",
       "      <th>question_mark_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10079</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15978</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15393 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000  00s   01   02   04   08   09   10  100  1000  ...  zuma  zurich  \\\n",
       "10079  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "15978  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "2151   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "5848   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "53     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "\n",
       "       zykina  ángel  íngrid  ürümqi  headline_length  punctuation%  \\\n",
       "10079     0.0    0.0     0.0     0.0             37.0          11.0   \n",
       "15978     0.0    0.0     0.0     0.0             36.0           0.0   \n",
       "2151      0.0    0.0     0.0     0.0             29.0           3.0   \n",
       "5848      0.0    0.0     0.0     0.0             59.0           0.0   \n",
       "53        0.0    0.0     0.0     0.0             42.0           2.0   \n",
       "\n",
       "       uppercase%  question_mark_count  \n",
       "10079        16.0                  0.0  \n",
       "15978        14.0                  0.0  \n",
       "2151         24.0                  0.0  \n",
       "5848         22.0                  0.0  \n",
       "53           29.0                  0.0  \n",
       "\n",
       "[5 rows x 15393 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5af74b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>00s</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>...</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zykina</th>\n",
       "      <th>ángel</th>\n",
       "      <th>íngrid</th>\n",
       "      <th>ürümqi</th>\n",
       "      <th>headline_length</th>\n",
       "      <th>punctuation%</th>\n",
       "      <th>uppercase%</th>\n",
       "      <th>question_mark_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15393 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000  00s   01   02   04   08   09   10  100  1000  ...  zuma  zurich  \\\n",
       "2991   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "14600  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "1781   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "2268   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "7160   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0     0.0   \n",
       "\n",
       "       zykina  ángel  íngrid  ürümqi  headline_length  punctuation%  \\\n",
       "2991      0.0    0.0     0.0     0.0             46.0           0.0   \n",
       "14600     0.0    0.0     0.0     0.0             49.0           0.0   \n",
       "1781      0.0    0.0     0.0     0.0             41.0           2.0   \n",
       "2268      0.0    0.0     0.0     0.0             56.0           0.0   \n",
       "7160      0.0    0.0     0.0     0.0             49.0           2.0   \n",
       "\n",
       "       uppercase%  question_mark_count  \n",
       "2991         20.0                  0.0  \n",
       "14600         4.0                  0.0  \n",
       "1781         22.0                  0.0  \n",
       "2268         25.0                  0.0  \n",
       "7160         20.0                  0.0  \n",
       "\n",
       "[5 rows x 15393 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "786a6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "rf = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Training the model\n",
    "rf_model = rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the values\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate the scores\n",
    "precision, recall, fscore, support = score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d1af655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.97088021 0.9419775 ] \n",
      "Recall: [0.93738019 0.97308869] \n",
      "Fscore: [0.95383615 0.95728039] \n",
      "Support: [1565 1635] \n",
      "Accuracy: 0.955625\n"
     ]
    }
   ],
   "source": [
    "accuracy = (y_pred == Y_test).sum()/len(y_pred)\n",
    "print(\"Precision: {} \\nRecall: {} \\nFscore: {} \\nSupport: {} \\nAccuracy: {}\". format(precision, recall, fscore, support, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba523f",
   "metadata": {},
   "source": [
    "## <center>Random forest model with grid search</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950dcc4b",
   "metadata": {},
   "source": [
    "<b>Grid search:</b> Searchs a combination of parameters to determine the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "718d394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(n_est, depth):\n",
    "    # Initializing the model\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    \n",
    "    # Training the model\n",
    "    rf_model = rf.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict the values\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    precision, recall, fscore, support = score(Y_test, y_pred, average = 'binary')\n",
    "    accuracy = (y_pred == Y_test).sum()/len(y_pred)\n",
    "    return precision, recall, fscore, support, accuracy\n",
    "    \n",
    "    # Print scores\n",
    "    # print(\"Est: {}, depth: {} ---> Precision: {} | Recall: {} | Fscore: {} | Support: {} | Accuracy: {}\". format(n_est, depth, precision, recall, fscore, support, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa1453fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metrics = []\n",
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        precision, recall, fscore, support, accuracy = train_rf(n_est, depth)\n",
    "        score_metrics.append((n_est, depth, precision, recall, fscore, support, accuracy ))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fba949b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimator</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "      <th>Support</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.809819</td>\n",
       "      <td>0.958410</td>\n",
       "      <td>0.877871</td>\n",
       "      <td>None</td>\n",
       "      <td>0.863750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.943987</td>\n",
       "      <td>0.700917</td>\n",
       "      <td>0.804493</td>\n",
       "      <td>None</td>\n",
       "      <td>0.825937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.911639</td>\n",
       "      <td>0.914985</td>\n",
       "      <td>0.913309</td>\n",
       "      <td>None</td>\n",
       "      <td>0.911250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.932143</td>\n",
       "      <td>0.957798</td>\n",
       "      <td>0.944796</td>\n",
       "      <td>None</td>\n",
       "      <td>0.942813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.826751</td>\n",
       "      <td>0.960245</td>\n",
       "      <td>0.888512</td>\n",
       "      <td>None</td>\n",
       "      <td>0.876875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.900578</td>\n",
       "      <td>0.952905</td>\n",
       "      <td>0.926003</td>\n",
       "      <td>None</td>\n",
       "      <td>0.922188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.955758</td>\n",
       "      <td>0.898471</td>\n",
       "      <td>0.926230</td>\n",
       "      <td>None</td>\n",
       "      <td>0.926875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.944214</td>\n",
       "      <td>0.973089</td>\n",
       "      <td>0.958434</td>\n",
       "      <td>None</td>\n",
       "      <td>0.956875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.969140</td>\n",
       "      <td>0.806728</td>\n",
       "      <td>0.880507</td>\n",
       "      <td>None</td>\n",
       "      <td>0.888125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.961133</td>\n",
       "      <td>0.892355</td>\n",
       "      <td>0.925468</td>\n",
       "      <td>None</td>\n",
       "      <td>0.926562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.936792</td>\n",
       "      <td>0.960856</td>\n",
       "      <td>0.948671</td>\n",
       "      <td>None</td>\n",
       "      <td>0.946875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.939269</td>\n",
       "      <td>0.974312</td>\n",
       "      <td>0.956470</td>\n",
       "      <td>None</td>\n",
       "      <td>0.954688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimator  max_depth  Precision    Recall    Fscore Support  Accuracy\n",
       "0            10       10.0   0.809819  0.958410  0.877871    None  0.863750\n",
       "1            10       20.0   0.943987  0.700917  0.804493    None  0.825937\n",
       "2            10       30.0   0.911639  0.914985  0.913309    None  0.911250\n",
       "3            10        NaN   0.932143  0.957798  0.944796    None  0.942813\n",
       "4            50       10.0   0.826751  0.960245  0.888512    None  0.876875\n",
       "5            50       20.0   0.900578  0.952905  0.926003    None  0.922188\n",
       "6            50       30.0   0.955758  0.898471  0.926230    None  0.926875\n",
       "7            50        NaN   0.944214  0.973089  0.958434    None  0.956875\n",
       "8           100       10.0   0.969140  0.806728  0.880507    None  0.888125\n",
       "9           100       20.0   0.961133  0.892355  0.925468    None  0.926562\n",
       "10          100       30.0   0.936792  0.960856  0.948671    None  0.946875\n",
       "11          100        NaN   0.939269  0.974312  0.956470    None  0.954688"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating dataframe of the score metrics\n",
    "score_df = pd.DataFrame(score_metrics, columns=['n_estimator', 'max_depth', 'Precision', 'Recall', 'Fscore', 'Support', 'Accuracy'])\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1101de7",
   "metadata": {},
   "source": [
    "## <center>Random Forest with Grid search cross validation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a775151",
   "metadata": {},
   "source": [
    "<b>Cross validation:</b> Divides the dataset into k number of sets and repeats the holdout mehod k number of times where a different set is heldout in each of the iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8433a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14516402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40.268153</td>\n",
       "      <td>0.382470</td>\n",
       "      <td>0.545211</td>\n",
       "      <td>0.014522</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.948434</td>\n",
       "      <td>0.954622</td>\n",
       "      <td>0.953250</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>79.799901</td>\n",
       "      <td>0.509921</td>\n",
       "      <td>0.863582</td>\n",
       "      <td>0.039970</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.951631</td>\n",
       "      <td>0.948247</td>\n",
       "      <td>0.952560</td>\n",
       "      <td>0.950812</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58.530479</td>\n",
       "      <td>0.327601</td>\n",
       "      <td>0.743737</td>\n",
       "      <td>0.089203</td>\n",
       "      <td>60</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 300}</td>\n",
       "      <td>0.944507</td>\n",
       "      <td>0.947309</td>\n",
       "      <td>0.945997</td>\n",
       "      <td>0.945938</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.166373</td>\n",
       "      <td>0.315735</td>\n",
       "      <td>0.494802</td>\n",
       "      <td>0.009083</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 150}</td>\n",
       "      <td>0.943382</td>\n",
       "      <td>0.946184</td>\n",
       "      <td>0.944309</td>\n",
       "      <td>0.944625</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.722114</td>\n",
       "      <td>0.252790</td>\n",
       "      <td>0.539590</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>30</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 300}</td>\n",
       "      <td>0.936258</td>\n",
       "      <td>0.942434</td>\n",
       "      <td>0.939246</td>\n",
       "      <td>0.939313</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "7      40.268153      0.382470         0.545211        0.014522   \n",
       "8      79.799901      0.509921         0.863582        0.039970   \n",
       "5      58.530479      0.327601         0.743737        0.089203   \n",
       "4      30.166373      0.315735         0.494802        0.009083   \n",
       "2      31.722114      0.252790         0.539590        0.034200   \n",
       "\n",
       "  param_max_depth param_n_estimators                                  params  \\\n",
       "7              90                150  {'max_depth': 90, 'n_estimators': 150}   \n",
       "8              90                300  {'max_depth': 90, 'n_estimators': 300}   \n",
       "5              60                300  {'max_depth': 60, 'n_estimators': 300}   \n",
       "4              60                150  {'max_depth': 60, 'n_estimators': 150}   \n",
       "2              30                300  {'max_depth': 30, 'n_estimators': 300}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "7           0.956693           0.948434           0.954622         0.953250   \n",
       "8           0.951631           0.948247           0.952560         0.950812   \n",
       "5           0.944507           0.947309           0.945997         0.945938   \n",
       "4           0.943382           0.946184           0.944309         0.944625   \n",
       "2           0.936258           0.942434           0.939246         0.939313   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "7        0.003508                1  \n",
       "8        0.001853                2  \n",
       "5        0.001145                3  \n",
       "4        0.001166                4  \n",
       "2        0.002522                5  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Setting the parameters for grid search\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90]}\n",
    "\n",
    "# Intializing grid search cv\n",
    "gs = GridSearchCV(rf, param, cv =3)\n",
    "gs_fit = gs.fit(features, target)\n",
    "\n",
    "# Create dataframe with ascending accuracy value (mean_test_score == accuracy)\n",
    "gs_df = pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec085c",
   "metadata": {},
   "source": [
    "## <center>Gradient Boosting with grid search</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "507e8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c57ea83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gb(n_est, depth, lr):\n",
    "    # Initializing the model\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth, learning_rate=lr)\n",
    "    \n",
    "    # Training the model\n",
    "    gb_model = gb.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict the values\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    precision, recall, fscore, support = score(Y_test, y_pred, average = 'binary')\n",
    "    accuracy = (y_pred == Y_test).sum()/len(y_pred)\n",
    "    return precision, recall, fscore, support, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a46216",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metrics = []\n",
    "for n_est in [50,100]:\n",
    "    for depth in [3,7]:\n",
    "        for lr in [0.1,1]:\n",
    "            precision, recall, fscore, support, accuracy = train_gb(n_est, depth,lr)\n",
    "            score_metrics.append((n_est, depth, lr, precision, recall, fscore, support, accuracy ))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe of the score metrics\n",
    "score_df = pd.DataFrame(score_metrics, columns=['n_estimator', 'max_depth', 'Learning Rate', 'Precision', 'Recall', 'Fscore', 'Support', 'Accuracy'])\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b29026",
   "metadata": {},
   "source": [
    "## <center>Gradient boosting with Grid search cross validation</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be760e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "# Setting the parameters for grid search\n",
    "param = {'n_estimators': [100, 150],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [1]}\n",
    "\n",
    "# Intializing grid search cv\n",
    "gs = GridSearchCV(gb, param, cv =3)\n",
    "gs_fit = gs.fit(features, target)\n",
    "\n",
    "# Create dataframe with ascending accuracy value (mean_test_score == accuracy)\n",
    "gs_df = pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e9a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
